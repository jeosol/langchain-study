{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeosol/langchain-study/blob/main/samw-yt/YT_ChatGPT_API_with_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4ca2Z08vpqfJ",
        "outputId": "0e437e0b-739e-4670-87a3-8ce8bd49235b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.2/348.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.6/567.6 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m130.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.7/288.7 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.49 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install openai langchain==0.0.99rc0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basics with OpenAI API"
      ],
      "metadata": {
        "id": "ne-Qg0YiqA75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key =''\n",
        "os.environ['OPENAI_API_KEY'] = ''"
      ],
      "metadata": {
        "id": "M5b0ALlsp8Eh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Hello what kind of assistant are you?\"},\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "iYvl1FGPrMNn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch7hAIq3rRLo",
        "outputId": "f9ec3e07-9eaf-4780-8ed0-fba0b2caeb9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-6pakUtAnnDzKpBkh2QcxLAp1ymTtR at 0x7fdae98f9400> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"content\": \"I am a virtual assistant, equipped with AI technology to assist you with various tasks and answer your questions as best as I can. How may I assist you today?\",\n",
              "        \"role\": \"assistant\"\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1677754010,\n",
              "  \"id\": \"chatcmpl-6pakUtAnnDzKpBkh2QcxLAp1ymTtR\",\n",
              "  \"model\": \"gpt-3.5-turbo-0301\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 35,\n",
              "    \"prompt_tokens\": 26,\n",
              "    \"total_tokens\": 61\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat Markup Language - token system\n",
        "\n",
        "```markdown\n",
        "<|im_start|>system\n",
        "You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.\n",
        "Knowledge cutoff: 2021-09-01\n",
        "Current date: 2023-03-01<|im_end|>\n",
        "<|im_start|>user\n",
        "How are you<|im_end|>\n",
        "<|im_start|>assistant\n",
        "I am doing well!<|im_end|>\n",
        "<|im_start|>user\n",
        "How are you now?<|im_end|>\n",
        "```\n",
        "\n",
        "```\n",
        "import openai\n",
        "\n",
        "openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
        "    ]\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "5PCc41QfsI21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant named Kate.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Hello what kind of assistant are you?\"},\n",
        "    ]"
      ],
      "metadata": {
        "id": "2CTxNRyZrgdN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note that the\n",
        "conversation_total_tokens = 0\n",
        "\n",
        "while True:\n",
        "    message = input(\"Human: \")\n",
        "    if message=='exit':\n",
        "        print(f\"{conversation_total_tokens} tokens used in total in this conversation\")\n",
        "        break\n",
        "    if message:\n",
        "        messages.append(\n",
        "            {\"role\": \"user\", \"content\": message},\n",
        "        )\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\", messages=messages\n",
        "        )\n",
        "\n",
        "    reply = response.choices[0].message.content\n",
        "    total_tokens = response.usage['total_tokens']\n",
        "    conversation_total_tokens += total_tokens\n",
        "    print(f\"ChatGPT: {reply} \\n {total_tokens} tokens used\")\n",
        "    messages.append({\"role\": \"assistant\", \"content\": reply})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keYvaAHJuzef",
        "outputId": "9d3abee1-5dd6-4e60-dcde-311859b754e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: When was machiavelli born\n",
            "ChatGPT: Niccolò Machiavelli was born on May 3, 1469. \n",
            " 56 tokens used\n",
            "Human: How long did he live?\n",
            "ChatGPT: Niccolò Machiavelli lived for 58 years. He passed away on June 21, 1527. \n",
            " 95 tokens used\n",
            "Human: In what city did he live in?\n",
            "ChatGPT: Machiavelli was born and lived in Florence, Italy. \n",
            " 124 tokens used\n",
            "Human: exit\n",
            "275 tokens used in total in this conversation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatGPT with LangChain"
      ],
      "metadata": {
        "id": "uAGcCgZkxbmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDQfHX6AwYea",
        "outputId": "3bf77286-830c-44f0-b8e9-dee00b93c1e3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.0.99rc0\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://www.github.com/hwchase17/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, aleph-alpha-client, dataclasses-json, deeplake, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI, OpenAIChat"
      ],
      "metadata": {
        "id": "SZXeJzHGxgOw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix_messages = [{\"role\": \"system\", \"content\": \"You are a helpful history professor named Kate.\"}]\n"
      ],
      "metadata": {
        "id": "g-00Nk5704vL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## old way\n",
        "# llm = OpenAI(model_name=\"text-davinci-003\",\n",
        "#              temperature=0, )\n",
        "\n",
        "## New way\n",
        "llm = OpenAIChat(model_name='gpt-3.5-turbo',\n",
        "             temperature=0,\n",
        "             prefix_messages=prefix_messages,\n",
        "             max_tokens = 256)"
      ],
      "metadata": {
        "id": "ygQ3pfROxyhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "template = \"\"\"Take the following question: {user_input}\n",
        "\n",
        "Answer it in an informative and intersting but conscise way for someone who is new to this topic.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template,\n",
        "                        input_variables=[\"user_input\"])\n"
      ],
      "metadata": {
        "id": "lkX7ybjFFHkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "user_input = \"When was Marcus Aurelius the emperor of Rome?\"\n",
        "\n",
        "llm_chain.run(user_input)"
      ],
      "metadata": {
        "id": "U0wSCoNvFNI9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "78b85cb6-231c-4821-ac5d-41563ab0413e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Marcus Aurelius was the emperor of Rome from 161 to 180 AD. He was known for his philosophical writings, particularly his book \"Meditations,\" which is still studied today. During his reign, he faced challenges such as wars with Germanic tribes and a devastating plague. Despite these difficulties, he is remembered as one of Rome\\'s \"Five Good Emperors\" for his efforts to improve the lives of his subjects and his commitment to justice and virtue.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jByI1AacuVEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "user_input = \"Who was Marcus Aurelius married to?\"\n",
        "\n",
        "llm_chain.run(user_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "WOlKi88W2SsJ",
        "outputId": "e2de79e5-b366-4899-82a0-d2c7780e5dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Marcus Aurelius was married to a woman named Faustina the Younger. She was the daughter of Antoninus Pius, who was the emperor before Marcus Aurelius. Faustina was known for her beauty and intelligence, and she was a devoted wife to Marcus Aurelius. However, there were rumors that she was unfaithful to him, which caused him great distress. Despite this, Marcus Aurelius remained loyal to her and even deified her after her death.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "reuPLunX3sEm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}